vnic_type="dpdk"
# if user_data has a value then cloud-init will be used and the user data will come from here
user_data="mime-data"
dns_server=""
# how many instances to run nfv workload
num_vm=1
# optionally specify the specific compute-node to start the instance, this is used when direct connection is made from trafficgen to the compute
#compute_node="compute-0.localdomain"
overcloudrc="/home/stack/overcloudrc"
stackrc="/home/stack/stackrc"
vm_image_name=nfv
nfv_tmp_dir=/tmp/nfv
vm_image_file=rhel.qcow2
vm_image_url=/home/stack/rhel-guest-image-7-4-191-x86-64-qcow2
# access_network_type: flat|vlan|shared
# shared: access network is sharing the same port as the data network but on different vlan
access_network_type=vlan
access_network_vlan=200
provider_network_type=flat
data_vlan_start=250
traffic_gen_remote_addr=""
traffic_gen=trex-txrx
exp_pbench_trafficgen=false
traffic_gen_src_slot=5e:00.2
traffic_gen_dst_slot=5e:00.3
traffic_src_mac=ac:1f:6b:2d:a9:f5
traffic_dst_mac=ac:1f:6b:2d:a9:f6
traffic_loss_pct=0.002
#data_pkt_size=64,256,512,1024,1500
data_pkt_size=64
search_runtime=30
validation_runtime=30
samples=1
num_flows=128
# traffic_direction: bidirec | unidirec | revunidirec
traffic_direction="bidirec"
repin_ovs_nonpmd=false
repin_kvm_emulator=false
# routing: vpp | testpmd | testpmd-sriov
routing="testpmd"
# testpmd_fwd: mac|io|macswap
testpmd_fwd="io"
run_traffic_gen=true
#traffic_gen_extra_opt="--skip-git-pull --flow-mods=src-ip --rate=5"
traffic_gen_extra_opt="--skip-git-pull --flow-mods=src-ip"
run_pbench=true
# stop pbench tool after test? if not, manually run pbench tool is possible after test
stop_pbench_after=false
# what pbench tool to run on the computes and VMs; it need to be a space seperated list
pbench_comupte_tools="proc-interrupts sar openvswitch"
pbench_vm_tools="proc-interrupts sar"
ftrace_host="n"
ftrace_vm="n"
# clean up instances after test?
cleanup=false
pbench_report_prefix="2hour-isolcpu-OSP12-direct-srcip_flows"
enable_HT=true
enable_multi_queue=false
repin_ovs_pmd=true
enable_spectre=false
# cat /sys/class/net/<nic>/device/numa_node to check numa node
# the core list may change based on the nic numa used
# keep in mind, pmd_vm_eth0/pmd_dpdk2 handles instance access port; 
# pmd_vm_eth1/pmd_dpdk0 pmd_vm_eth2/pmd_dpdk1 handles data port
pmd_vm_eth0=3
pmd_vm_eth1=1
pmd_vm_eth2=2
pmd_dpdk0=33
pmd_dpdk1=34
pmd_dpdk2=35
spare_cores=""
